%%%% CAPÍTULO 2 - REVISÃO DA LITERATURA (OU REVISÃO BIBLIOGRÁFICA, ESTADO DA ARTE, ESTADO DO CONHECIMENTO)
%%
%% O autor deve registrar seu conhecimento sobre a literatura básica do assunto, discutindo e comentando a informação já publicada.
%% A revisão deve ser apresentada, preferencialmente, em ordem cronológica e por blocos de assunto, procurando mostrar a evolução do tema.
%% Título e rótulo de capítulo (rótulos não devem conter caracteres especiais, acentuados ou cedilha)
\chapter{Materiais}\label{cap:fundamentacao}

\section{Extração de Características}
As características utilizadas para treinar os modelos foram escolhidas da seguinte maneira, por meio de pesquisa, conversa com alunos e professores e experiencias previas em atividades na faculdade, levantamos 3 tipos de características que poderiam ser uteis para o treinamento dos modelos, estas foram: Histograma em \gls{rgb}, em \gls{hsi} e o \gls{hog}, mas ao realizarmos os teste obtivemos que o \gls{hog} ruins, e o \gls{hsi} era levemente inferior ao \gls{rgb}, testamos também utilizando o \gls{rgb} em conjunto com os outros dois, mas também obtivemos resultados ruins, então concluímos que utilização somente da característica do histograma \gls{rgb} era nossa melhor opção.

Depois de determinarmos que o histograma \gls{rgb} era a melhor característica para este problema, testamos mais de um metodo de sua utilização, acabamos utilizando o histograma em 16 \textit{bins} por canal de cor, ou seja, conseguimos 48 características para cada instancia.

\subsection{Pré-processamento das imagens}

Antes da extração dos descritores, cada imagem passa pelo seguinte processo:
\textbf{Conversão para RGB}, o uso do formato RGB padroniza os canais cromáticos, uma vez que algumas imagens podem estar em tons de cinza ou paletas indexadas; \textbf{Redimensionamento para 96×96 pixels} Definido em \textbf{IMG\_SIZE = (96, 96)}, esse tamanho reduzido permite a diminuição significativa do custo computacional, normalização da dimensão entre classes, preservação suficiente dos detalhes faciais e do contorno dos personagens; \textbf{Conversão para array NumPy} Necessário para cálculos de histogramas.


\subsection{Histogramas RGB Normalizados}
A extração de informações sobre cor são essenciais neste problema, esse é o motivo da utilização dos histogramas \gls{rgb} pois os personagens dos Simpsons possuem uma identidade cromática forte:

\begin{table}[h!]
	\centering
	\caption{Características cromáticas dos Simpsons}
	\label{tabela-3.1}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Personagem} & \textbf{Características cromáticas}\\
		\hline
		Bart   & camiseta vermelha, pele amarela vibrante, cabelo amarelo irregular \\
%		\hline
		Homer   & camisa branca, barba azul clara \\
%		\hline
		Marge   & cabelo azul alto, vestido verde \\
%		\hline
		Lisa   & vestido vermelho, colar branco \\
%		\hline
		Maggie   & roupa azul-bebê, laço azul \\
		\hline
	\end{tabular}
\end{table}	

	Essas cores são informação altamente discriminativa, especialmente considerando que as imagens foram redimensionalizadas para uma baixa resolução (96×96).

\subsubsection{16 bins por canal e normalização}
Um histograma de 16 bins por canal é um compromisso entre: granularidade suficiente para distinguir tons, dimensionalidade moderada (8×3 = 24 atributos), robustez a ruídos e variação de iluminação.

A normalização: $ h = h / (h.sum() + 1e-8) $ garante que variações de exposição, iluminação ou brilho não distorçam os histogramas.

\subsection{Descritor \gls{rgb}}
Depois de calcular o histograma de cada um dos três canais, concatenamos cada canal em somente um, assim tendo 48 características. Na prática, esse descritor permite que modelos simples (como KNN ou Decision Tree) já capturem boa parte das diferenças visuais entre classes, enquanto modelos mais complexos (SVM, MLP e Random Forest) exploram melhor a riqueza estrutural do vetor.

.

\subsection{Relação entre as Características e os Resultados Obtidos}
Observando os resultados das matrizes de confusão (arquivos enviados), é possível verificar que:
\begin{itemize}
	\item Classes com padrões de cor muito distintos (ex.: Marge e Maggie) tendem a se beneficiar do histograma RGB.
	\item Personagens visualmente mais próximos (ex.: Homer × Bart ou Lisa × Maggie) apresentaram confusões previsíveis, mitigadas parcialmente pelo ensemble.
\end{itemize}

Assim, o uso combinado de HOG + histogramas de cor foi fundamental para permitir que os classificadores produzissem resultados consistentes mesmo em uma base pequena e com baixa resolução.

\section{Classificadores Avaliados}
Para a etapa de classificação das características extraídas (e Histogramas de Cor), foram selecionados cinco algoritmos distintos, visando explorar diferentes comportamentos estatísticos. Os modelos escolhidos foram:

\begin{itemize}
	\item \textbf{K-Nearest Neighbors (KNN)}: Um classificador baseado em instância que rotula amostras com base na classe majoritária entre os k vizinhos mais próximos.
	\item 	\textbf{Support Vector Machine (SVM}): Focado em encontrar o hiperplano ótimo de separação em espaços de alta dimensão.
	\item 	\textbf{Decision Tree (DT)}: Um modelo baseado em regras de inferência hierárquicas, capaz de capturar relações não lineares simples.
	\item 	\textbf{Random Forest (RF)}: Um método de bagging que combina múltiplas árvores de decisão para reduzir a variância e o risco de overfitting.
	\item 	\textbf{Multilayer Perceptron (MLP)}: Uma rede neural artificial feedforward que utiliza retropropagação para aprender mapeamentos complexos entre as características visuais e as classes dos personagens.
\end{itemize}

\subsection{Análise Comparativa dos Classificadores}
Os experimentos demonstraram que a tarefa de classificação dos personagens apresenta desafios significativos, principalmente devido à natureza das características extraídas e ao possível desequilíbrio entre as classes.

Observa-se que, embora o SVM e o Random Forest tenham alcançado acurácias razoáveis (acima de 47\%), seus valores de F1-Score Macro foram baixos (0,395 e 0,266, respectivamente). Isso indica um viés dos modelos em favor das classes majoritárias. A análise das matrizes de confusão confirma esse comportamento:

\begin{itemize}
	\item O Random Forest classificou corretamente 33 das 35 amostras da classe majoritária (Classe 0), mas falhou completamente em identificar qualquer amostra das Classes 2, 3 e 4 (Recall = 0 para estas classes).
	\item O SVM apresentou comportamento similar, com alta precisão global, mas baixa capacidade de recuperação (Recall) para as classes minoritárias.
\end{itemize}

Por outro lado, o MLP e o KNN demonstraram um equilíbrio maior entre precisão e revocação, sacrificando um pouco da acurácia global para manter a capacidade de detectar as classes menos representadas, resultando em F1-Scores superiores aos métodos baseados em árvores.

Além dos classificadores individuais, foi implementada uma estratégia de Ensemble Learning utilizando um Soft Voting Classifier. Este ensemble combina as probabilidades preditas por 20 estimadores base (variações dos algoritmos citados acima com diferentes hiperparâmetros) para computar a classe final, buscando maior robustez e generalização.

Modelo final combinado:
\begin{itemize}
	\item 	5 KNNs
	\item 	5 SVMs
	\item 	4 Random Forests
	\item 	3 Decision Trees
	\item 	3 MLPs
\end{itemize}

Em votação \textbf{soft}, utilizando probabilidades médias.


\subsection{Otimização de Hiperparâmetros e Validação Cruzada}
Para garantir que os resultados não fossem enviesados por uma única divisão de dados ou por configurações arbitrárias, adotou-se um protocolo rigoroso de validação.

O processo de treino e validação seguiu a técnica de Validação Cruzada Estratificada (Stratified K-Fold) com \textbf{k=10}. Esta técnica garante que a proporção de classes em cada dobra seja preservada, o que é crucial dado o possível desbalanceamento entre as classes dos personagens.

Para cada algoritmo individual, foi executada uma busca exaustiva de hiperparâmetros (Grid Search), otimizando a métrica \textbf{F1-Score Macro}. As grades de busca definidas foram:

\begin{table}[h!]
	\centering
	\caption{Tabela de Hiperparâmetros}
	\label{tabela-3.1}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Modelo} & Hiperparâmetros\\
		\hline
		Decision Tree   & k={3, 5, 7, 9}Pesos: ‘uniform’ e ‘distance’ \\
		\hline
		Random Forest   & C={1, 10, 100} Gamma: ‘scale’, 1\^-3 e 1\^-4 Kernel: ‘rbf’ \\
		\hline
		KNN   & 45,26\% \\
		\hline
		SVM (RBF)   & 53,68\% \\
		\hline
		MLP   & 51,58\% \\
		\hline
	\end{tabular}
\end{table}	

